{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\brace\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import random\n",
    "from numpy import argmax\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOADING PRETRAINED WORD EMBEDDING\n",
    "\n",
    "We load the glove pretrained word embeddings which have embeddings for all the words in the english vocabulary, we use this list of words also to remove from the dataset every word that is not in the english vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READING DATA FROM CSV FILE\n",
    "\n",
    "We use a dataset consisting of approximatly 4000 articles with their summarization, here we read this data from a csv file and we do not consider articles that are too short (less than 5 words), articles that are long (more than 300 words, we do not consider this articles to ease the training process) and articles that are duplicates. We save this data in a dictionary and we then print how large the dataset that we will use is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2366\n"
     ]
    }
   ],
   "source": [
    "# reading the dataset from a csv file\n",
    "with open('news_summary.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    # create a dictionary where we will store pairs (complete text/ summary)\n",
    "    dataset = dict()\n",
    "    # loop through every line in the csv file\n",
    "    for row in csv_reader:\n",
    "            # if the current line is not the first one (descriptions of fields) and it is not a duplicate, we update the dictionary\n",
    "            # we also remove texts that are too long to ease the training process\n",
    "            if row[3] != 'read_more' and row[3] not in dataset and len(row[5].split()) < 300:\n",
    "                # push pairs of input text / summaries to dictionary (the key is the url of the text), we are removing really short input\n",
    "                if len(row[5].split()) < 5 or len(row[4].split()) < 3:\n",
    "                    continue\n",
    "                dataset[row[3]] = list()\n",
    "                dataset[row[3]].append(row[4])\n",
    "                dataset[row[3]].append(row[5])\n",
    "print(len(dataset))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING THE TEXTS\n",
    "\n",
    "We clean the initial articles and the related summaries by removing punctuation and making all the characters lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_descriptions(descriptions):\n",
    "    # loop through all keys in dictionary\n",
    "    for key, text_list in descriptions.items():\n",
    "        # loop through the full text and summary associated to a given key\n",
    "        for i in range(len(text_list)):\n",
    "            # transform current sentence to array of words\n",
    "            text = text_list[i].split()\n",
    "            # convert all words to lower case\n",
    "            text = [word.lower() for word in text]\n",
    "            # remove punctuation from each word\n",
    "            text = [w.translate(str.maketrans('', '', string.punctuation)) for w in text]\n",
    "            # remove all the stopwords\n",
    "            text = [w for w in text if not w in set(stopwords.words('english'))]\n",
    "            # remove all the words that are not in the english dictionary\n",
    "            text = [w for w in text if embeddings_index.get(w) is not None]\n",
    "            # store cleaned descriptions as string and we need to add a startseq and endseq token\n",
    "            text_list[i] =  'startseq ' + ' '.join(text) + ' endseq'\n",
    "            \n",
    "# remove punctuation from text and make all text lowercase \n",
    "clean_descriptions(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE TOKENIZER AND FIND SIZES USEFUL FOR DEFINING THE MODEL\n",
    "\n",
    "Here we fit a tokenizer to our dataset, afterwards we use this tokenizer object to find the size of our vocabulary (which is equal to the number of unique words in our dataset, we will need this value to know the size of the softmax output layer for our model). We also find the maximum length of input text and summary, this values are also used for defining the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The complete vocabulary size is 24255\n",
      "Maximum full text Length: 194\n",
      "Maximum summary Length: 47\n"
     ]
    }
   ],
   "source": [
    "# get a list from dictionary of data to be fed into tokenizer\n",
    "listFromDictTot = list()\n",
    "# append all full texts and summaries to the list\n",
    "for key in dataset.keys():\n",
    "        [listFromDictTot.append(d) for d in dataset[key]]\n",
    "# tokenize all words (the machine learning model is going to need numbers as input)\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(listFromDictTot)\n",
    "# get the size of the vocabulary created with our dataset (unique words in dataset)\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "print(\"The complete vocabulary size is \" + str(vocabulary_size))\n",
    "# get the maximum length of a full text and of a summary in the dataset (size needed when creating model to train)\n",
    "max_summary_length = max(len(listFromDictTot[i].split()) for i in range(0,len(listFromDictTot),2))\n",
    "max_length = max(len(listFromDictTot[i].split()) for i in range(1,len(listFromDictTot),2))\n",
    "print('Maximum full text Length: %d' % max_length)\n",
    "print('Maximum summary Length: %d' % max_summary_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN/TEST SPLIT\n",
    "\n",
    "Here we split the dataset in train and test data (we create 2 dictionaries containing this data), in the end of the block we also check the size of the training and test dataset to make sure that everything worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the training dataset is 1892\n",
      "The length of the test dataset is 474\n"
     ]
    }
   ],
   "source": [
    "# get the keys\n",
    "keys = list(dataset.keys())\n",
    "# reorder the keys randomly\n",
    "random.shuffle(keys)\n",
    "# set the percentage of data that will be used for training\n",
    "trainPercentage = 0.8\n",
    "# get the last index for which the data will be assigned to the training test\n",
    "trainLastIndex = int(trainPercentage * len(keys))\n",
    "trainDataset = dict()\n",
    "# fill training dictionary\n",
    "for i in range(trainLastIndex):\n",
    "    trainDataset[keys[i]] = dataset[keys[i]]\n",
    "testDataset = dict()\n",
    "# fill test dictionary\n",
    "for i in range(trainLastIndex, len(keys)):\n",
    "    testDataset[keys[i]] = dataset[keys[i]]\n",
    "# print size of the train and test data to make sure that everything worked well\n",
    "print(\"The length of the training dataset is \" + str(len(trainDataset)))\n",
    "print(\"The length of the test dataset is \" + str(len(testDataset))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRANSFORM THE DATA IN A BETTER FORMAT\n",
    "\n",
    "Here we transform the dictionary containing the training data into two lists, one contains the summaries and the other the complete text. The text is transformed to a sequence of tokens and is padded to the maximum lengths before being inserted in the relative lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputTextDataset = list()\n",
    "targetSummaryDataset = list()\n",
    "# append all full texts and summaries to the list\n",
    "for key in trainDataset.keys():\n",
    "        inputTextDataset.append(tf.keras.preprocessing.sequence.pad_sequences([tokenizer.texts_to_sequences([trainDataset[key][1]])[0]], maxlen = max_length, padding='post'))\n",
    "        targetSummaryDataset.append(tf.keras.preprocessing.sequence.pad_sequences([tokenizer.texts_to_sequences([trainDataset[key][0]])[0]], maxlen = max_summary_length, padding='post'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINE GAN MODEL\n",
    "\n",
    "Here we define a GAN model where the generator is biderectional LSTM that takes as input a full text and produces a summary, this is then fed with real summaries to generator (which uses a CNN, that is usually considered a good model for text classification) that is supposed to distinguish between artificial and real summaries. Unfortunately we did not succeed in stabilize the training, therefore we did not obtain good results (mostly because of the short time and computing resources available), we tried some techniques including adding a minibatch standard deviation layer in the end of the discriminator, using batch normalization and substituting ReLU with LeakyReLU, we believe that the discriminator is training much faster than generator, forcing the generator to stop learning because of the lack of incentives. Given the problems in stabilizing the adversarial training we opted for an encoder decoder architecture which is defined in the next box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 200\n",
    "embedding_dim=110\n",
    "batch_size = 16\n",
    "\n",
    "# mini-batch standard deviation layer\n",
    "class MinibatchStdev(tf.keras.layers.Layer):\n",
    "    # initialize layer\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MinibatchStdev, self).__init__(**kwargs)\n",
    "    # perform the operation\n",
    "    def call(self, inputs):\n",
    "        # size of group of which standard deviation is to be computed\n",
    "        group_size = tf.shape(inputs)[0]\n",
    "        shape = list(tf.keras.backend.int_shape(inputs))\n",
    "        shape[0] = tf.shape(inputs)[0]\n",
    "        minibatch = tf.keras.backend.reshape(inputs,(group_size, -1, shape[1], shape[2]))\n",
    "        # substracts the mean from every element\n",
    "        minibatch -= tf.reduce_mean(minibatch, axis=0, keepdims=True)\n",
    "        # compute the square of the means from the previous results\n",
    "        minibatch = tf.reduce_mean(tf.keras.backend.square(minibatch), axis = 0)\n",
    "        # get sqrt of the previous results\n",
    "        minibatch = tf.keras.backend.sqrt(minibatch + 1e8)\n",
    "        minibatch = tf.reduce_mean(minibatch, keepdims=True)\n",
    "        # tile output to get wanted size\n",
    "        minibatch = tf.keras.backend.tile(minibatch,[group_size, 1, shape[2]])\n",
    "        # add the result of the computation to the input\n",
    "        return tf.keras.backend.concatenate([inputs, minibatch], axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # create a copy of the input shape as a list\n",
    "        input_shape = list(input_shape)\n",
    "        # add one to the channel dimension where we will have the standard deviation\n",
    "        input_shape[-1] += 1\n",
    "        # convert list to a tuple\n",
    "        return tuple(input_shape)\n",
    "\n",
    "# use sequential API to define generator model    \n",
    "generator = tf.keras.models.Sequential()\n",
    "# dropout layer for regulaziation\n",
    "generator.add(tf.keras.layers.Dropout(0.5))\n",
    "# biderectional lstm which will return the entire sequence\n",
    "generator.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(150, input_shape=(max_length, vocabulary_size), return_sequences=True)))\n",
    "# time distributed layer to deal with the entire sequence of outputs and classifying correct word with dense softmax layer\n",
    "generator.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(vocabulary_size, activation='softmax')))\n",
    "# we crop the output in order to have the wanted dimension\n",
    "generator.add(tf.keras.layers.Cropping1D(cropping=(max_length - max_summary_length,0)))\n",
    "\n",
    "# use sequential API to define discriminator model  \n",
    "discriminator = tf.keras.models.Sequential()\n",
    "# use batch normalization because it is usually helpful in GAN training\n",
    "discriminator.add(tf.keras.layers.BatchNormalization())\n",
    "# one dimensional convolutional layer\n",
    "discriminator.add(tf.keras.layers.Conv1D(64, 5,  input_shape=[max_summary_length, vocabulary_size]))\n",
    "# using leakyrelu becausee it proved good results for gans\n",
    "discriminator.add(tf.keras.layers.LeakyReLU(alpha=0.02))\n",
    "# dropout layer for normalization\n",
    "discriminator.add(tf.keras.layers.Dropout(0.5))\n",
    "# one dimensional convolutional layer\n",
    "discriminator.add(tf.keras.layers.Conv1D(128, 5))\n",
    "# using leakyrelu becausee it proved good results for gans\n",
    "discriminator.add(tf.keras.layers.LeakyReLU(alpha=0.02))\n",
    "# adding minibatch standard deviation layer to give the generator an incentive to generate different summaries\n",
    "discriminator.add(MinibatchStdev())\n",
    "# sigmoid layer for binary classification (real vs artificial summary)\n",
    "discriminator.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# combining the generator and the discriminator to create the gan\n",
    "gan = tf.keras.models.Sequential([generator, discriminator])\n",
    "# compile the discriminator using binary crossentropy (binary classification) and adam optimizer\n",
    "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "# making the discriminator untrainable and comÂ´pile the entire model (for the training of the generator)\n",
    "discriminator.trainable = False\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINING THE ENCODER-DECODER MODEL\n",
    "\n",
    "This encoder decoder architecture embedds the input tokens (from the complete texts) and then feed this data into an LSTM, we then use the final state of this LSTM as the initial state for the decoder model which is another LSTM, also here we embedd the input tokens (which now are the summaries) before passing them to an LSTM, after this LSTM we place a dense layer with softmax activation function that outputs a probability distribution over the vocabulary. We use the glove pretrained word embeddings in order to make training easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_281\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_158 (InputLayer)          [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_159 (InputLayer)          [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_95 (Embedding)        (None, None, 100)    2425500     input_158[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_96 (Embedding)        (None, None, 100)    2425500     input_159[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_54 (Dropout)            (None, None, 100)    0           embedding_95[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_55 (Dropout)            (None, None, 100)    0           embedding_96[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_44 (LSTM)                  [(None, 320), (None, 538880      dropout_54[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_45 (LSTM)                  [(None, None, 320),  538880      dropout_55[0][0]                 \n",
      "                                                                 lstm_44[0][1]                    \n",
      "                                                                 lstm_44[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, None, 24255)  7785855     lstm_45[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 13,714,615\n",
      "Trainable params: 8,863,615\n",
      "Non-trainable params: 4,851,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "\n",
    "# size of embedding as defined in the glove model\n",
    "embedding_size = 100\n",
    "lstm_dim = 320\n",
    "\n",
    "#create a matrix that contains that weights for our vocabulary as we found in the glove pretrained model\n",
    "embedding_matrix = np.zeros((vocabulary_size, embedding_size))\n",
    "# loops through all the words in our dictionary\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    # get the embedding vector associated to the word\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in will be zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "        \n",
    "# define encoder model using the functional API               \n",
    "encoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
    "# we use an embedding layer which has the weights found in the pretrained glove model (this layer will not be trained)\n",
    "embeddingEncoder = tf.keras.layers.Embedding(vocabulary_size, embedding_size, weights=[embedding_matrix], trainable=False)(encoder_inputs)\n",
    "# dropout layer for regularization\n",
    "embeddingEncoder = tf.keras.layers.Dropout(0.5)(embeddingEncoder)\n",
    "# lstm layer where we set return_state to true because we are interested in its final state (initial state of the decoder)\n",
    "encoder = tf.keras.layers.LSTM(lstm_dim, return_state=True)\n",
    "# state_h is the short term final state of the lstm while state_c is the long term final state of the lstm\n",
    "_, state_h, state_c = encoder(embeddingEncoder)\n",
    "# saving final encoder states\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# define encoder model using the functional API\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
    "# we use an embedding layer which has the weights found in the pretrained glove model (this layer will not be trained)\n",
    "final_dex = tf.keras.layers.Embedding(vocabulary_size, embedding_size, weights=[embedding_matrix], trainable=False)(decoder_inputs)\n",
    "# dropout layer for regularization\n",
    "final_dex = tf.keras.layers.Dropout(0.5)(final_dex)\n",
    "# lstm layer where we set return_state to true because we are interested in its final state, as it will be the state used to predict the next word, we also need the output in this case\n",
    "decoder_lstm = tf.keras.layers.LSTM(lstm_dim, return_state=True, return_sequences=True)\n",
    "# the decoder output is now the output of the model (at the current timestep)\n",
    "decoder_outputs, _, _ = decoder_lstm(final_dex, initial_state=encoder_states)\n",
    "# after we will need a dense layer with softmax activation to generate the probability distribution over the vocabulary\n",
    "decoder_dense = tf.keras.layers.Dense(vocabulary_size, activation='softmax') \n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "# now we create the model that we will use with encoder and decoder inputs, and decoder outputs\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "# we compile the model using rmsprop and categorical_crossentropy as the loss function\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAKING THE DATA READY FOR TRAINING\n",
    "\n",
    "Here we simply create a dataset object by combining complete text and summaries, this dataset is then shuffled and batched, after these steps we are ready to start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data to a dataset object and shuffle it\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputTextDataset, targetSummaryDataset)).shuffle(1000)\n",
    "# we batch and prefetch (start filling buffer used during training in background thread) the dataset\n",
    "dataset = dataset.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING FOR ENCODER DECODER\n",
    "\n",
    "Here we train he encoder decoder model, we use this loop in order to not have to store all the one hot outputs in one large matriz considering that the vocabulary size is quiet large (almost 40000 entries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_encoder_decoder(dataset, batch_size, n_epochs = 50):\n",
    "    epochCounter = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"The current epoch is \" + str(epochCounter))\n",
    "        print(decode_seq(inputSentence))\n",
    "        epochCounter += 1\n",
    "        # use batches to make the one hot encoded outputs take less space\n",
    "        for encoder_input_batch, decoder_input_batch in dataset:\n",
    "            # reshape input for encoder (full sentence) and decoder (short version) to two dimensional tensors\n",
    "            encoder_input_batch = tf.reshape(encoder_input_batch, [encoder_input_batch.get_shape()[0], encoder_input_batch.get_shape()[2]])\n",
    "            decoder_input_batch = tf.reshape(decoder_input_batch, [decoder_input_batch.get_shape()[0], decoder_input_batch.get_shape()[2]])\n",
    "            decoder_output = list()\n",
    "            # create the decoder output by shifting the decoder input by one\n",
    "            for i in range(encoder_input_batch.get_shape()[0]):\n",
    "                for j in range(max_summary_length):\n",
    "                    if j == 0:\n",
    "                        decoder_output.append(list())  \n",
    "                        continue\n",
    "                    decoder_output[i].append(decoder_input_batch[i][j])\n",
    "                # we append a final zero to get matching sizes    \n",
    "                decoder_output[i].append(0)\n",
    "            # we make the outputs one hot encoded vectors    \n",
    "            decoder_output = tf.one_hot(decoder_output, vocabulary_size)\n",
    "            model.fit([encoder_input_batch, decoder_input_batch], decoder_output, verbose=0)\n",
    "                 \n",
    "# train the encoder-decoder model\n",
    "train_encoder_decoder(dataset, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAN TRAINING\n",
    "\n",
    "Here is our attempt at training the GAN model, we start by generating summaries for the current batch using the generator, we then use this summaries (which will have label 0) and the real summaries (which will have label 1) to train the discriminator. Afterwards we set the discriminator to not trainable and we train the entire GAN (only the weights of the generator can change) using labels 1 for the outputs of the genarator, therefore trying to make the generator convince the discriminator that the output that it is producing are real summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(dataset, batch_size, n_epochs = 5):\n",
    "    generator, discriminator = gan.layers\n",
    "    epochCounter = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"The current epoch is \" + str(epochCounter))\n",
    "        epochCounter += 1\n",
    "        for X_batch, Y_batch in dataset:\n",
    "            # discriminator training\n",
    "            # we reshape the X_batch to a two dimensional tensor\n",
    "            X_batch = tf.reshape(X_batch, [X_batch.get_shape()[0], X_batch.get_shape()[2]])\n",
    "            # we use one hot encoding to get the representation expected by the model\n",
    "            X_batch = tf.one_hot(X_batch, vocabulary_size)\n",
    "            # we generate fake summaries using the current generator\n",
    "            generated_summaries = generator(X_batch)\n",
    "            # reshaping Y batch to 2 dimensions and then using one hot encoding to get correct model input format\n",
    "            Y_batch = tf.reshape(Y_batch, [Y_batch.get_shape()[0], Y_batch.get_shape()[2]])\n",
    "            Y_batch = tf.one_hot(Y_batch, vocabulary_size)\n",
    "            # we concatenate real and fake summaries (which will be input to the discriminator)\n",
    "            X_fake_and_real = tf.concat( [generated_summaries, Y_batch], axis=0)\n",
    "            # we give a label of 0 (fake) to the artificially generated summaries and a label of 1 (real) to the real summaries\n",
    "            y1 = tf.constant([[0.]] * int(X_fake_and_real.get_shape()[0] / 2) + [[1.]] * int(X_fake_and_real.get_shape()[0] / 2) )\n",
    "            # we now make the dicriminator trainable and then we train it\n",
    "            discriminator.trainable = True\n",
    "            discriminator.train_on_batch(X_fake_and_real, y1)\n",
    "            # here we assign a label of 1 (real) to the artificially generated summaries and then we train the generator (the discriminator is now made untrainable)\n",
    "            y2 = tf.constant([[1.]] * int(X_fake_and_real.get_shape()[0] / 2))\n",
    "            discriminator.trainable = False\n",
    "            gan.train_on_batch(X_batch, y2) # substitute noise with the corrent x_input\n",
    "                 \n",
    "# train the GAN model\n",
    "train_gan(dataset, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUNNING INFERENCE WITH THE GENERATOR PART OF THE GAN MODEL\n",
    "\n",
    "We use the generator from the gan model that we defined to generate summaries from input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function recovers the original word from a token (if there is a translation)\n",
    "def id_to_word(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "# we get the generator from the gan model\n",
    "generator, _ = gan.layers\n",
    "# we transform an input text to the correct format\n",
    "inputSentence = tf.keras.preprocessing.sequence.pad_sequences([tokenizer.texts_to_sequences([testDataset[list(testDataset.keys())[3]][0]])[0]], maxlen = max_length, padding='post')\n",
    "# we run inference and then get a two dimensional shape\n",
    "summary = generator(tf.one_hot(inputSentence, vocabulary_size)) # generate summary\n",
    "summary = tf.reshape(summary, [summary.get_shape()[1], summary.get_shape()[2]])\n",
    "finalSummary = \"\"\n",
    "# we transform the indexes to words\n",
    "for i in range(len(summary)):\n",
    "    currentWord = id_to_word(argmax(summary[i]), tokenizer)\n",
    "    if currentWord==\"startseq\":\n",
    "        continue\n",
    "    elif currentWord==\"endseq\":\n",
    "        break\n",
    "    else:\n",
    "        finalSummary = finalSummary + \" \" + currentWord "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUNNING INFERENCE WITH THE ENCODER DECODER MODEL\n",
    "\n",
    "We use the encoder decoder architecture trained previously to run inference on an input sequence of words (which is padded), we use a greedy approach to picking the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder-decoder model for inference\n",
    "# link encoder inference to the encoder layers that we trained\n",
    "encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "# create a placeholder for the decoder input states\n",
    "decoder_state_input_h = tf.keras.layers.Input(shape=(lstm_dim,))\n",
    "decoder_state_input_c = tf.keras.layers.Input(shape=(lstm_dim,))\n",
    "decoder_state_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "# link the previously trained decoder input layer to the decoder inference model\n",
    "inference_decoder = tf.keras.layers.Embedding(vocabulary_size, embedding_size, weights=[embedding_matrix])(decoder_inputs)\n",
    "# we link the previously trained lstm decoder weights to our inference decoder\n",
    "inference_decoder_outputs, state_h2, state_c2 = decoder_lstm(inference_decoder, initial_state=decoder_state_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "# we link the final decoder layer to the inference model and then we combine the decoder layers in a single model\n",
    "inference_decoder_outputs = decoder_dense(inference_decoder_outputs)\n",
    "decoder_model = tf.keras.models.Model([decoder_inputs] + decoder_state_inputs, [inference_decoder_outputs] + decoder_states2)\n",
    "\n",
    "\n",
    "def decode_seq(input_seq):\n",
    "    # get the final states from the encoder\n",
    "    state_values = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # get the startseq token to initialize inference\n",
    "    target_seq[0,0] = tokenizer.texts_to_sequences(['startseq'])[0][0]\n",
    "    decoded_sentence = ''\n",
    "    for i in range(max_summary_length):\n",
    "        # run inference for the current word given the states that we are in\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + state_values)\n",
    "        # get the word with the highest probability at the current step\n",
    "        sampled_token_index = np.argmax(output_tokens[0,-1,:])\n",
    "        sampled_char = id_to_word(sampled_token_index, tokenizer)\n",
    "        # if the word is not in the vocabulary we break\n",
    "        if sampled_char == None:\n",
    "            break\n",
    "        # if we reached the end of the sentence we break    \n",
    "        if(sampled_char == 'endseq'):\n",
    "            stop_condition = True    \n",
    "        # we add the word to our prediction    \n",
    "        decoded_sentence += ' ' + sampled_char\n",
    "        # update state input and word input with current state output and word output\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0,0] = sampled_token_index\n",
    "        state_values = [h,c] \n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVALUATION \n",
    "\n",
    "We evaluate the dataset using BLEU-1 score and we do this by using the inference function described above. We ran evaluation using the test dataset which we set aside from the entire dataset in the beginning of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(dataset):\n",
    "    # actual keeps track of correct sequence while predicted is the predicted sequence\n",
    "    actual, predicted = list(), list()\n",
    "    # looping through every key in dataset\n",
    "    for key, texts in dataset.items():\n",
    "        # prediction for current text\n",
    "        prediction = decode_seq(tf.keras.preprocessing.sequence.pad_sequences([tokenizer.texts_to_sequences([texts[1]])[0]], maxlen = max_length, padding='post'))\n",
    "        # get the correct descriptions and remove the startseq and endseq tokens\n",
    "        correctSummary = texts[0].split()[1:-1]\n",
    "        # append the prediction and the correct summary to the lists\n",
    "        actual.append(correctSummary)\n",
    "        predicted.append(prediction.split())\n",
    "    # calculate BLEU score for 1 GRAM\n",
    "    print(\"Bleu-1 score: \" + str(corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))))\n",
    "    \n",
    "evaluate_model(testDataset)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
