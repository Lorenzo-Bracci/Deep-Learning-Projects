{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utility modules\n",
    "from os import listdir\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "from numpy import array\n",
    "from numpy import argsort\n",
    "import string\n",
    "# import tensorflow modules\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Masking\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "# import bleu module (for evaluation)\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "EXTRACTING FEATURE VECTORS FROM IMAGES\n",
    "\n",
    "In the blocks below we define a function called extract_feature_vectors that takes as an input the directoy containing all the images in our dataset and then runs inference on all these images using the ResNet50 pre-trained model offered by tensorflow.keras. We remove the last layer of the model (softmax layer), in this way the model will be able to extract useful features (which will be input to an LSTM later on) from the images which will then be stored in a pickle file so that we can have access to these features vectors later on without the need to recompute them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_vectors(directory):\n",
    "    model = ResNet50()\n",
    "    # remove the softmax layer\n",
    "    model.layers.pop()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    # dictionary where we store the feature vectors\n",
    "    featuresDict = dict()\n",
    "    # used to keep track of the progress\n",
    "    counter = 0\n",
    "    # iterate through all the images in the current directory (all images in Flickr8k)\n",
    "    for name in listdir(directory):\n",
    "        # load current image and convert pixels to array\n",
    "        filename = directory + '/' + name\n",
    "        # the target size is 224x224 because it is the expected size for resnet50\n",
    "        image = load_img(filename, target_size=(224, 224))\n",
    "        image = img_to_array(image)\n",
    "        # reshape data to four dimension as expected by resnet50\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        # using resnet preprocessing\n",
    "        image = preprocess_input(image)\n",
    "        # run inference with our version of resnet50 (missing the softmax layer), we have verbose=0 to not show progress\n",
    "        feature = model.predict(image, verbose=0)\n",
    "        # get image name from filename (cut out the .jpg)\n",
    "        image_id = name.split('.')[0]\n",
    "        # insert feature in features dictionary\n",
    "        featuresDict[image_id] = feature\n",
    "        # shows progress\n",
    "        print(counter)\n",
    "        counter += 1\n",
    "    return featuresDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory where we can find all images in Flickr8k\n",
    "imagedirectory = \"C:\\\\Users\\\\brace\\\\Downloads\\\\Flickr8k_Dataset\\\\Flicker8k_Dataset\"\n",
    "# using function defined above to run resnet50 on all images\n",
    "features = extract_feature_vectors(imagedirectory)\n",
    "# dump feature dictionary python object to a file to use it in the future without having to run resnet50 again\n",
    "dump(features, open('features.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESS ALL THE DESCRIPTIONS\n",
    "\n",
    "In the following block we preprocess a file containig the descriptions for all the images in the dataset (5 descriptions per image). We do this by making all the descriptions lowercase and removing punctuations, all this data will be stored in a dictionary where the keys are the image names and the values are lists of clean descriptions. We will then store this information in a file in order to be able to access it later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 8092 \n"
     ]
    }
   ],
   "source": [
    "def descriptionsToDict(doc):\n",
    "    # create a dictionary where keys are imageNames and values are a list of 5 descriptions\n",
    "    imageToDescription = dict()\n",
    "    # process lines\n",
    "    for line in doc.split('\\n'):\n",
    "        # get all words by splitting line by white space\n",
    "        words = line.split()\n",
    "        # words should have at least an imagename and a word for descritions\n",
    "        if len(words) < 2:\n",
    "            continue\n",
    "        # take the first word as the image name and remove .jpg\n",
    "        image_name = words[0].split('.')[0]\n",
    "        # convert description words array to string (no image name)\n",
    "        image_desc = ' '.join(words[1:])\n",
    "        # create the list if imagename is not already in the dictionary\n",
    "        if image_name not in imageToDescription:\n",
    "            imageToDescription[image_name] = list()\n",
    "        # add current description to dictionary\n",
    "        imageToDescription[image_name].append(image_desc)\n",
    "    return imageToDescription\n",
    "\n",
    "def clean_descriptions(descriptions):\n",
    "    # loop through all keys in dictionary\n",
    "    for key, desc_list in descriptions.items():\n",
    "        # loop through all sentences associated to a given key\n",
    "        for i in range(len(desc_list)):\n",
    "            # transform current sentence to array of words\n",
    "            desc = desc_list[i].split()\n",
    "            # convert all words to lower case\n",
    "            desc = [word.lower() for word in desc]\n",
    "            # remove punctuation from each word\n",
    "            desc = [w.translate(str.maketrans('', '', string.punctuation)) for w in desc]\n",
    "            # store cleaned descriptions as string\n",
    "            desc_list[i] =  ' '.join(desc)\n",
    "\n",
    "\n",
    "def saveToFile(descriptions, filename):\n",
    "    lines = list()\n",
    "    # loop through all descriptions\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            # add descriton to list\n",
    "            lines.append(key + ' ' + desc)\n",
    "    # store the clean descriptions in a file\n",
    "    file = open(filename, 'w')\n",
    "    file.write('\\n'.join(lines))\n",
    "    file.close()\n",
    "\n",
    "# file with all descriptions for all images\n",
    "descriptionFile = \"C:\\\\Users\\\\brace\\\\Downloads\\\\Flickr8k_text\\\\Flickr8k.token.txt\"\n",
    "# load descriptions from descriptionFile\n",
    "file = open(descriptionFile, 'r')\n",
    "descriptions = file.read()\n",
    "file.close()\n",
    "# create a dictionary with the descriptions\n",
    "descriptions = descriptionsToDict(descriptions)\n",
    "print('Loaded: %d ' % len(descriptions))\n",
    "# clean descriptions\n",
    "clean_descriptions(descriptions)\n",
    "# save to file\n",
    "saveToFile(descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD THE TRAINING DATA\n",
    "\n",
    "In this box we load both the feature vectors as well as the preprocessed descriptions for the training dataset (the training dataset has already been created by the owners of the dataset, which created a txt file with the names of all images that are in the training dataset). We will also add a startseq as well as an endseq token to all descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pre-defined list of photos\n",
    "def load_dataset(filename):\n",
    "    # read data from file\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    dataset = list()\n",
    "    for line in text.split('\\n'):\n",
    "        # do not consider empty lines\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        # append current name without .jpg\n",
    "        dataset.append(line.split('.')[0])\n",
    "    # make sure that there is no duplicate    \n",
    "    return set(dataset)\n",
    "\n",
    "def filter_processed_descriptions(filename, dataset):\n",
    "    # read data from file\n",
    "    file = open(filename, 'r')\n",
    "    doc = file.read()\n",
    "    file.close()\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        # get all words in a line\n",
    "        words = line.split()\n",
    "        # get descrition without imageName\n",
    "        image_desc = words[1:]\n",
    "        # check if name is in current dataset\n",
    "        if words[0] in dataset:\n",
    "            # if this is first description for given imageName we create list\n",
    "            if words[0] not in descriptions:\n",
    "                descriptions[words[0]] = list()\n",
    "            # add start and end words to descrition before storing it in dictionary\n",
    "            descriptions[words[0]].append('startseq ' + ' '.join(image_desc) + ' endseq')\n",
    "    return descriptions\n",
    "\n",
    "def filter_image_features(filename, dataset):\n",
    "    # load all features from pickle file\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    # filter features\n",
    "    features = {k: all_features[k] for k in dataset}\n",
    "    return features\n",
    "\n",
    "# dataset with names of images in training dataset\n",
    "filename = \"C:\\\\Users\\\\brace\\\\Downloads\\\\Flickr8k_text\\\\Flickr_8k.trainImages.txt\"\n",
    "train = load_dataset(filename)\n",
    "# load descriptions and image feaures for training dataset\n",
    "train_descriptions = filter_processed_descriptions('descriptions.txt', train)\n",
    "train_features = filter_image_features('features.pkl', train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAKING ALL THE DATA READY TO BE FED INTO THE MODEL\n",
    "\n",
    "Here we fit a tokenizer on our training dataset in order to transform the words to numerical values (which will later on be transformed in work embeddings), we also compute the number of different words that are present in the dataset (this information will be needed in the softmax layer after tha LSTM) as well as the maximum size of description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum description Length: 38\n"
     ]
    }
   ],
   "source": [
    "# get a list from dictionary of training descritption to be fed into tokenizer\n",
    "listFromDict = list()\n",
    "# append all the descriptions to the list\n",
    "for key in train_descriptions.keys():\n",
    "        [listFromDict.append(d) for d in train_descriptions[key]]\n",
    "# tokenize all words (the machine learning model is going to need numbers as input)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(listFromDict)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# get the maximum length of a descrition in the training dataset (size needed when creating model to train)\n",
    "max_length = max(len(d.split()) for d in listFromDict)\n",
    "print('Maximum description Length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE GENERATOR WHICH IS USED TO FEED THE DATA TO THE MODEL DURING TRAINING\n",
    "\n",
    "In the following code we create a generator that at every iteration will yield the feature vector for the input image (which is the first input to the LSTM) and a list of sequence of tokenized words (input to a given time step) as well as the correct output word (in tokenized form) for the specific time step which is simply the next word in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequences of images features, input sequences and output words for an image\n",
    "def create_sequences(desc_list, imageFeature, tokenizer, max_length):\n",
    "    inputImage, inputText, output = list(), list(), list()\n",
    "    # loop through every description for specific image\n",
    "    for desc in desc_list:\n",
    "        # encode the current description to a sequence of numeric values using a predefined tokenier which was fit on the training data\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        # split one sequence into multiple X,y pairs\n",
    "        for i in range(1, len(seq)): \n",
    "            # pad input sequence for them to have all the same size (the same as max_length)\n",
    "            currInput = pad_sequences([seq[:i]], maxlen=max_length)[0]\n",
    "            # transform output value to categorical (a row of the size of the vocabulary where all entries are 0 except from the entry corresponding to seq[i])\n",
    "            currOutput = to_categorical([seq[i]], num_classes=vocab_size)[0]\n",
    "            inputImage.append(imageFeature)\n",
    "            inputText.append(currInput)\n",
    "            output.append(currOutput)\n",
    "    return array(inputImage), array(inputText), array(output)\n",
    "\n",
    "def data_generator(descriptions, imageFeatures, tokenizer, max_length):\n",
    "        # loop through all key value pairs in the descritions (we have while 1 to always keep generating data)\n",
    "    while 1:   \n",
    "        for key, descriptionList in descriptions.items():\n",
    "            # retrieve the feature vector for the current image\n",
    "            imageFeature = imageFeatures[key][0]\n",
    "            # get tokenized input/output sequences for every step in the lstm, they should also have attachedinput image\n",
    "            inputImage, inputText, output = create_sequences(descriptionList, imageFeature, tokenizer, max_length)\n",
    "            #  yield is used to obtain the result of the current iteration of the generator\n",
    "            yield ((inputImage, inputText), output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE LSTM MODEL\n",
    "\n",
    "This model has two sources of input, the first one is the feature vector that we previously computed from an image using ResNet50, this input will be passed through a dropout layer and it will then be made of the same size as the word embeddings through a fully connected layer (which uses ReLU activation function). We then add the time dimension to this vector and we concatenate it to the second source of input which is the sequences of tokenized words (which has been transformed to a word embeddings through an embedding layer). We add another dropout layer before passing this input to an LSTM layer, we then have a dense layer with a softmax activation function which is used to generate the probability of the next word for each step. We use categorical crossentropy as a cost function and we use Adam as the optimizer. We will then train  the model for 50 epochs using batch size of 16 and the final model will be saved in a file called finalModel.h5 which will later on be used for evaluation and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the captioning model\n",
    "def create_model(vocab_size, max_length):\n",
    "    # this is the connection to the last layer of the CNN (whose output has size 1000)\n",
    "    inputFromCNN = Input(shape=(1000,))\n",
    "    # we use a dropout layer because in the paper it was suggested that it improves performance (we also use a small training dataset)\n",
    "    dropoutCNN = Dropout(0.5)(inputFromCNN)\n",
    "    # we use a fully connected layer to make the output of the CNN have the same size as the word embeddings\n",
    "    CNNToEmbeddings = Dense(256, activation='relu')(dropoutCNN)\n",
    "    # add time dimension so that this layer output shape is (None, 1, embed_size), we also need to use masking to avoid size errors\n",
    "    final_cnn = Masking()(RepeatVector(1)(CNNToEmbeddings))\n",
    "    # we create an input source from where we take the tokenized words, the max_length is the length of the longest caption\n",
    "    inputWords = Input(shape=(max_length,))\n",
    "    # we embedd the tokenized words to vectors of the size of the vocabulary\n",
    "    wordsEmbedding = Embedding(vocab_size, 256, mask_zero=True)(inputWords)\n",
    "    # we concatenate theoutput from the CNN to the word embeddings, in this way the first step of the LSTM will be fed with\n",
    "    # the output of the CNN and the other steps will be fed with word embeddings\n",
    "    concateateInputs = concatenate([final_cnn, wordsEmbedding], axis=1)\n",
    "    # an additional droupout layer for further regularization\n",
    "    droupoutLSTM = Dropout(0.5)(concateateInputs)\n",
    "    # we have an LSTM as specified in the paper, the input size is 256 (size of the embeddings)\n",
    "    LSTMLayer = LSTM(256)(droupoutLSTM)\n",
    "    # after the LSTM we have a fully connected layer (softmax activations) to output a probability distribution over the vocabulary size \n",
    "    outputs = Dense(vocab_size, activation='softmax')(LSTMLayer)\n",
    "    # set inputs (CNN output and words) and outputs (correct word) for the model \n",
    "    model = Model(inputs=[inputFromCNN, inputWords], outputs=outputs)\n",
    "    # we compile the model using categorical_crossentropy and adam optimizer\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    # visualize model\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_75\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_49 (InputLayer)           [(None, 1000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 1000)         0           input_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 256)          256256      dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_8 (RepeatVector)  (None, 1, 256)       0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_50 (InputLayer)           [(None, 38)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_8 (Masking)             (None, 1, 256)       0           repeat_vector_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 38, 256)      1954048     input_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 39, 256)      0           masking_8[0][0]                  \n",
      "                                                                 embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 39, 256)      0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, 256)          525312      dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 7633)         1961681     lstm_8[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 4,697,297\n",
      "Trainable params: 4,697,297\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 154s 411ms/step - loss: 5.5863\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 146s 389ms/step - loss: 4.7833\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 4.3805\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 145s 387ms/step - loss: 4.2185\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 145s 387ms/step - loss: 4.0463\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 145s 386ms/step - loss: 4.0505\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 143s 380ms/step - loss: 3.9915\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 154s 410ms/step - loss: 4.0963\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 195s 521ms/step - loss: 3.9766\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 251s 670ms/step - loss: 3.8622\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 271s 722ms/step - loss: 3.7499\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 351s 936ms/step - loss: 3.7922\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 369s 984ms/step - loss: 3.7520\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 340s 906ms/step - loss: 3.7011\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 334s 890ms/step - loss: 3.5119\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 330s 880ms/step - loss: 3.5779\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 428s 1s/step - loss: 3.4428\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 431s 1s/step - loss: 3.4672\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 265s 706ms/step - loss: 3.3402\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 228s 608ms/step - loss: 3.2924\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 146s 390ms/step - loss: 3.2446\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 145s 386ms/step - loss: 3.3046\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 143s 382ms/step - loss: 3.2674\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 144s 385ms/step - loss: 3.3795\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 141s 376ms/step - loss: 3.3351\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 144s 383ms/step - loss: 3.2736\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 141s 377ms/step - loss: 3.2013\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 145s 387ms/step - loss: 3.2486\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 3.2439\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 145s 387ms/step - loss: 3.2207\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 142s 380ms/step - loss: 3.0845\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 145s 387ms/step - loss: 3.1616\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 143s 381ms/step - loss: 3.1020\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 143s 380ms/step - loss: 3.1320\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 144s 384ms/step - loss: 3.0191\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 142s 379ms/step - loss: 2.9678\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 144s 384ms/step - loss: 2.9681\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 142s 379ms/step - loss: 3.0137\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 144s 385ms/step - loss: 2.9786\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 144s 383ms/step - loss: 3.0827\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 144s 383ms/step - loss: 3.0635\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 3.0125\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 142s 379ms/step - loss: 2.9497\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 143s 381ms/step - loss: 2.9908\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 142s 379ms/step - loss: 2.9963\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 146s 390ms/step - loss: 2.9815\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 148s 394ms/step - loss: 2.8734\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 152s 406ms/step - loss: 2.9481\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 148s 394ms/step - loss: 2.9048\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 163s 435ms/step - loss: 2.9249\n"
     ]
    }
   ],
   "source": [
    "# use function defined above to create the model\n",
    "model = create_model(vocab_size, max_length)\n",
    "# train the model, \n",
    "generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n",
    "model.fit(generator, epochs=50, batch_size=16, steps_per_epoch=len(train_descriptions)/16, verbose=1)\n",
    "model.save('finalModel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFERENCE USING BEAM SEARCH\n",
    "\n",
    "Instead of using the more simple greedy search apprach to inference (every time taking the word with highest probability as the next word) we implemented beam search in order to be able to experiment with different beam sizes (trying beamsize=1 would be the same as using grid search). Beam search simply means that you always consider the top n results (where n is the beam size), using this technique our results improve of approximatly 1-2 points for bleu-4 compared to the greedy approach even when the beam size was rather small (3-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function recovers the original word from a token (if there is a translation)\n",
    "def word_to_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "def beam_search_predictions(model, tokenizer, image, max_len, beam_index):\n",
    "    # create a token for the startseq\n",
    "    start = tokenizer.texts_to_sequences(['startseq'])[0]\n",
    "    # create a matrix where we keep track of the best scores\n",
    "    start_word = [[start, 0.0]]\n",
    "    # we make sure that we do not perform inference more times than the size of the longest sequence\n",
    "    while len(start_word[0][0]) < max_len:\n",
    "        # temporary array used to update the matrix with the best values\n",
    "        temp = []\n",
    "        # loop through all our current best results\n",
    "        for s in start_word:\n",
    "            # we pad our current sequence to max_length\n",
    "            paddedSeq = pad_sequences([s[0]], maxlen=max_len, padding='post')\n",
    "            # we run inference using the image and the current sequence (predict next word)\n",
    "            predictions = model.predict([image,paddedSeq], verbose=0)\n",
    "            # we get the top predictions from the probability distribution over the vocabulary\n",
    "            bestPredictions = argsort(predictions[0])[-beam_index:]\n",
    "            # we add the best predictions into our count for best predictions\n",
    "            for word in bestPredictions:\n",
    "                # get current sequence and probability\n",
    "                currentSeq, prob = s[0][:], s[1]\n",
    "                # append current word to sequence\n",
    "                currentSeq.append(word)\n",
    "                # update probability\n",
    "                prob += predictions[0][word]\n",
    "                # append to changes that need to be committed\n",
    "                temp.append([currentSeq, prob])\n",
    "        # commit changes to structure that keeps track of best result            \n",
    "        start_word = temp\n",
    "        # Sorting according to the probabilities\n",
    "        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n",
    "        # Getting the top words\n",
    "        start_word = start_word[-beam_index:]\n",
    "    \n",
    "    # get the best prediction\n",
    "    bestPrediction = start_word[-1][0]\n",
    "    # retrieve actual sentence from tokens\n",
    "    intermediate_caption = [word_to_id(i, tokenizer) for i in bestPrediction]\n",
    "\n",
    "    final_caption = []\n",
    "    # get all the words in the predicted caption that do not include start and end token\n",
    "    for i in intermediate_caption:\n",
    "        if i != 'endseq':\n",
    "            final_caption.append(i)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    final_caption = ' '.join(final_caption[1:])\n",
    "    return final_caption\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVALUATE THE MODEL AGAINST THE TEST DATASET\n",
    "\n",
    "Here we evaluate the model on the test dataset (already separeted by the creators of the dataset), we use beam search with a beam size of 5 and we use the preprepared features vectors for the images (which have already been stored in a pickle file). For evaluation metrics we use bleu scores as suggested in the paper (bleu-1, bleu-2, bleu-3 and bleu-4). We make sure that we have removed the startseq and endseq tokens from both predicted and correct output sentences in order to have a more realistic result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    # actual keeps track of correct sequence while predicted is the predicted sequence\n",
    "    actual, predicted = list(), list()\n",
    "    # looping through every image in dataset\n",
    "    for key, desc_list in descriptions.items():\n",
    "        # prediction for current image\n",
    "        prediction = beam_search_predictions(model, tokenizer, photos[key], max_length, 5)\n",
    "        # get the correct descriptions and remove the startseq and endseq tokens\n",
    "        correctDescritions = [d.split()[1:-1] for d in desc_list]\n",
    "        # append the prediction and the correct description to the lists\n",
    "        actual.append(correctDescritions)\n",
    "        predicted.append(prediction.split())\n",
    "    # calculate BLEU score for 1 ,2 ,3 and 4 GRAM\n",
    "    print(\"Bleu-1 score: \" + str(corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))))\n",
    "    print(\"Bleu-2 score: \" + str(corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))))\n",
    "    print(\"Bleu-3 score: \" + str(corpus_bleu(actual, predicted, weights=(0.33, 0.33, 0.33, 0))))\n",
    "    print(\"Bleu-4 score: \" + str(corpus_bleu(actual, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu-1 score: 0.4576165986877106\n",
      "Bleu-2 score: 0.2725993061426533\n",
      "Bleu-3 score: 0.17187589262983294\n",
      "Bleu-4 score: 0.1080040416947022\n"
     ]
    }
   ],
   "source": [
    "# load test dataset (descriptions)\n",
    "testDataset = \"C:\\\\Users\\\\brace\\\\Downloads\\\\Flickr8k_text\\\\Flickr_8k.testImages.txt\"\n",
    "test = load_dataset(testDataset)\n",
    "test_descriptions = filter_processed_descriptions('descriptions.txt', test)\n",
    "# load test dataset (pre-processed image features)\n",
    "test_features = filter_image_features('features.pkl', test)\n",
    "# load the model and evaluate using the test dataset\n",
    "model = load_model('finalModel.h5')\n",
    "evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFERENCE USING THE TRAINED MODEL\n",
    "\n",
    "Here we run inference on an image (that does not necessairily have to be in the Flickr8K dataset), we do this by first extracting the feature vector from the image and then using beam search (using the model that we trained above), we chose 5 for the beam size as it gives satisfactory results and it runs relatevly fast. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a group of people standing in front of a mountain\n"
     ]
    }
   ],
   "source": [
    "# we define a function to generate a feature vector from a specific image\n",
    "def generateFeatureVector(filename):\n",
    "    model = ResNet50()\n",
    "    # remove the kast layer of the ResNet50 model\n",
    "    model.layers.pop()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    image = load_img(filename, target_size=(224, 224))\n",
    "    # preprocess the image\n",
    "    image = img_to_array(image)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    image = preprocess_input(image)\n",
    "    # return feature vector computed with inference\n",
    "    return model.predict(image, verbose=0)\n",
    "\n",
    "photoPath = \"C:\\\\Users\\\\brace\\\\Downloads\\\\Flickr8k_Dataset\\\\Flicker8k_Dataset\\\\3222055946_45f7293bb2.jpg\"\n",
    "# get the feature vector from the image\n",
    "photo = generateFeatureVector(photoPath)\n",
    "# generate description and print it\n",
    "description = beam_search_predictions(model, tokenizer, photo, max_length, 5)\n",
    "print(description)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
