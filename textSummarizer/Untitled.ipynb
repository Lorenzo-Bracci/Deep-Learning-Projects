{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import random\n",
    "from numpy import argmax\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READING DATA FROM CSV FILE\n",
    "\n",
    "We use a dataset consisting of approximatly 4000 articles with their summarization, here we read this data from a csv file and we do not consider articles that are too short (less than 5 words), articles that are long (more than 300 words, we do not consider this articles to ease the training process) and articles that are duplicates. We save this data in a dictionary and we then print how large the dataset that we will use is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2366\n"
     ]
    }
   ],
   "source": [
    "# reading the dataset from a csv file\n",
    "with open('news_summary.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    # create a dictionary where we will store pairs (complete text/ summary)\n",
    "    dataset = dict()\n",
    "    # loop through every line in the csv file\n",
    "    for row in csv_reader:\n",
    "            # if the current line is not the first one (descriptions of fields) and it is not a duplicate, we update the dictionary\n",
    "            # we also remove texts that are too long to ease the training process\n",
    "            if row[3] != 'read_more' and row[3] not in dataset and len(row[5].split()) < 300:\n",
    "                # push pairs of input text / summaries to dictionary (the key is the url of the text), we need to add a startseq and endseq token\n",
    "                # we are removing really short input\n",
    "                if len(row[5].split()) < 5 or len(row[4].split()) < 3:\n",
    "                    continue\n",
    "                dataset[row[3]] = list()\n",
    "                dataset[row[3]].append('startseq ' + row[4] + ' endseq')\n",
    "                dataset[row[3]].append('startseq ' + row[5] + ' endseq')\n",
    "print(len(dataset))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING THE TEXTS\n",
    "\n",
    "We clean the initial articles and the related summaries by removing punctuation and making all the characters lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\brace\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "def clean_descriptions(descriptions):\n",
    "    # loop through all keys in dictionary\n",
    "    for key, text_list in descriptions.items():\n",
    "        # loop through the full text and summary associated to a given key\n",
    "        for i in range(len(text_list)):\n",
    "            # transform current sentence to array of words\n",
    "            text = text_list[i].split()\n",
    "            # convert all words to lower case\n",
    "            text = [word.lower() for word in text]\n",
    "            # remove punctuation from each word\n",
    "            text = [w.translate(str.maketrans('', '', string.punctuation)) for w in text]\n",
    "            text = [w for w in text if not w in set(stopwords.words('english'))]\n",
    "            # store cleaned descriptions as string\n",
    "            text_list[i] =  ' '.join(text)\n",
    "            \n",
    "# remove punctuation from text and make all text lowercase \n",
    "clean_descriptions(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE TOKENIZER AND FIND SIZES USEFUL FOR DEFINING THE MODEL\n",
    "\n",
    "Here we fit a tokenizer to our dataset, afterwards we use this tokenizer object to find the size of our vocabulary (which is equal to the number of unique words in our dataset, we will need this value to know the size of the softmax output layer for our model). We also find the maximum length of input text and summary, this values are also used for defining the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The complete vocabulary size is 38568\n",
      "Maximum full text Length: 210\n",
      "Maximum summary Length: 49\n"
     ]
    }
   ],
   "source": [
    "# get a list from dictionary of data to be fed into tokenizer\n",
    "listFromDictTot = list()\n",
    "# append all full texts and summaries to the list\n",
    "for key in dataset.keys():\n",
    "        [listFromDictTot.append(d) for d in dataset[key]]\n",
    "# tokenize all words (the machine learning model is going to need numbers as input)\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(listFromDictTot)\n",
    "# get the size of the vocabulary created with our dataset (unique words in dataset)\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "print(\"The complete vocabulary size is \" + str(vocabulary_size))\n",
    "# get the maximum length of a full text and of a summary in the dataset (size needed when creating model to train)\n",
    "max_summary_length = max(len(listFromDictTot[i].split()) for i in range(0,len(listFromDictTot),2))\n",
    "max_length = max(len(listFromDictTot[i].split()) for i in range(1,len(listFromDictTot),2))\n",
    "print('Maximum full text Length: %d' % max_length)\n",
    "print('Maximum summary Length: %d' % max_summary_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN/TEST SPLIT\n",
    "\n",
    "Here we split the dataset in train and test data (we create 2 dictionaries containing this data), in the end of the block we also check the size of the training and test dataset to make sure that everything worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the training dataset is 1892\n",
      "The length of the test dataset is 474\n"
     ]
    }
   ],
   "source": [
    "# get the keys\n",
    "keys = list(dataset.keys())\n",
    "# reorder the keys randomly\n",
    "random.shuffle(keys)\n",
    "# set the percentage of data that will be used for training\n",
    "trainPercentage = 0.8\n",
    "# get the last index for which the data will be assigned to the training test\n",
    "trainLastIndex = int(trainPercentage * len(keys))\n",
    "trainDataset = dict()\n",
    "# fill training dictionary\n",
    "for i in range(trainLastIndex):\n",
    "    trainDataset[keys[i]] = dataset[keys[i]]\n",
    "testDataset = dict()\n",
    "# fill test dictionary\n",
    "for i in range(trainLastIndex, len(keys)):\n",
    "    testDataset[keys[i]] = dataset[keys[i]]\n",
    "# print size of the train and test data to make sure that everything worked well\n",
    "print(\"The length of the training dataset is \" + str(len(trainDataset)))\n",
    "print(\"The length of the test dataset is \" + str(len(testDataset))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRANSFORM THE DATA IN A BETTER FORMAT\n",
    "\n",
    "Here we transform the dictionary containing the training data into two lists, one contains the summaries and the other the complete text. The text is transformed to a sequence of tokens and is padded to the maximum lengths before being inserted in the relative lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputTextDataset = list()\n",
    "targetSummaryDataset = list()\n",
    "# append all full texts and summaries to the list\n",
    "for key in trainDataset.keys():\n",
    "        inputTextDataset.append(tf.keras.preprocessing.sequence.pad_sequences([tokenizer.texts_to_sequences([trainDataset[key][1]])[0]], maxlen = max_length, padding='post'))\n",
    "        targetSummaryDataset.append(tf.keras.preprocessing.sequence.pad_sequences([tokenizer.texts_to_sequences([trainDataset[key][0]])[0]], maxlen = max_summary_length, padding='post'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINE GAN MODEL\n",
    "\n",
    "Here we define a GAN model where the generator is biderectional LSTM that takes as input a full text and produces a summary, this is then fed with real summaries to generator (which uses a CNN, that is usually considered a good model for text classification) that is supposed to distinguish between artificial and real summaries. Unfortunately we did not succeed in stabilize the training, therefore we did not obtain good results (mostly because of the short time and computing resources available), we tried some techniques including adding a minibatch standard deviation layer in the end of the discriminator, using batch normalization and substituting ReLU with LeakyReLU, we believe that the discriminator is training much faster than generator, forcing the generator to stop learning because of the lack of incentives. Given the problems in stabilizing the adversarial training we opted for an encoder decoder architecture which is defined in the next box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 200\n",
    "embedding_dim=110\n",
    "batch_size = 16\n",
    "\n",
    "# mini-batch standard deviation layer\n",
    "class MinibatchStdev(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MinibatchStdev, self).__init__(**kwargs)\n",
    "    # perform the operation\n",
    "    def call(self, inputs):\n",
    "        group_size = tf.shape(inputs)[0]\n",
    "        shape = list(tf.keras.backend.int_shape(inputs))\n",
    "        shape[0] = tf.shape(inputs)[0]\n",
    "        minibatch = tf.keras.backend.reshape(inputs,(group_size, -1, shape[1], shape[2]))\n",
    "        minibatch -= tf.reduce_mean(minibatch, axis=0, keepdims=True)\n",
    "        minibatch = tf.reduce_mean(tf.keras.backend.square(minibatch), axis = 0)\n",
    "        minibatch = tf.keras.backend.square(minibatch + 1e8)\n",
    "        minibatch = tf.reduce_mean(minibatch, keepdims=True)\n",
    "        minibatch = tf.keras.backend.tile(minibatch,[group_size, 1, shape[2]])\n",
    "        return tf.keras.backend.concatenate([inputs, minibatch], axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # create a copy of the input shape as a list\n",
    "        input_shape = list(input_shape)\n",
    "        # add one to the channel dimension (assume channels-last)\n",
    "        input_shape[-1] += 1\n",
    "        # convert list to a tuple\n",
    "        return tuple(input_shape)\n",
    "\n",
    "    \n",
    "generator = tf.keras.models.Sequential()\n",
    "generator.add(tf.keras.layers.Dropout(0.5))\n",
    "# generator.add(tf.keras.layers.LSTM(150, input_shape=(max_length, embedding_dim),  batch_input_shape=[batch_size], stateful=True))\n",
    "# generator.add(tf.keras.layers.RepeatVector(max_length))\n",
    "generator.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(150, input_shape=(max_length, vocabulary_size), return_sequences=True, stateful=False)))\n",
    "generator.add(tf.keras.layers.Dropout(0.5))\n",
    "generator.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(150, input_shape=(max_length, vocabulary_size), return_sequences=True, stateful=False)))\n",
    "generator.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(vocabulary_size, activation='softmax')))\n",
    "generator.add(tf.keras.layers.Cropping1D(cropping=(max_length - max_summary_length,0)))\n",
    "\n",
    "\n",
    "discriminator = tf.keras.models.Sequential()\n",
    "discriminator.add(tf.keras.layers.BatchNormalization())\n",
    "discriminator.add(tf.keras.layers.Conv1D(64, 5,  input_shape=[max_summary_length, vocabulary_size], batch_size=batch_size))\n",
    "discriminator.add(tf.keras.layers.LeakyReLU(alpha=0.02))\n",
    "discriminator.add(tf.keras.layers.Dropout(0.5))\n",
    "discriminator.add(tf.keras.layers.Conv1D(128, 5))\n",
    "discriminator.add(tf.keras.layers.LeakyReLU(alpha=0.02))\n",
    "discriminator.add(MinibatchStdev())\n",
    "discriminator.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "gan = tf.keras.models.Sequential([generator, discriminator])\n",
    "# compile the model\n",
    "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "discriminator.trainable = False\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "#generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINING THE ENCODER-DECODER MODEL\n",
    "\n",
    "This encoder decoder architecture embedds the input tokens (from the complete texts) and then feed this data into an LSTM, we then use the final state of this LSTM as the initial state for the decoder model which is another LSTM, also here we embedd the input tokens (which now are the summaries) before passing them to an LSTM, after this LSTM we place a dense layer with softmax activation function that outputs a probability distribution over the vocabulary. We use the glove pretrained word embeddings in order to make training easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(None, 210)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           [(None, 49)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 210, 100)     3856800     input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 49, 100)      3856800     input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 210, 100)     0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 49, 100)      0           embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   [(None, 320), (None, 538880      dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   [(None, 49, 320), (N 538880      dropout_8[0][0]                  \n",
      "                                                                 lstm_6[0][1]                     \n",
      "                                                                 lstm_6[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 49, 38568)    12380328    lstm_7[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 21,171,688\n",
      "Trainable params: 13,458,088\n",
      "Non-trainable params: 7,713,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "\n",
    "embedding_size = 100\n",
    "lstm_dim = 320\n",
    "embedding_matrix = np.zeros((vocabulary_size, embedding_size))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "encoder_inputs = tf.keras.layers.Input(shape=(max_length))\n",
    "en_x = tf.keras.layers.Embedding(vocabulary_size, embedding_size, weights=[embedding_matrix], input_length=max_length, trainable=False)(encoder_inputs)\n",
    "en_x = tf.keras.layers.Dropout(0.5)(en_x)\n",
    "encoder = tf.keras.layers.LSTM(lstm_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(en_x)\n",
    "encoder_states = [state_h, state_c]\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(max_summary_length))\n",
    "final_dex = tf.keras.layers.Embedding(vocabulary_size, embedding_size, weights=[embedding_matrix], input_length=max_summary_length, trainable=False)(decoder_inputs)\n",
    "final_dex = tf.keras.layers.Dropout(0.5)(final_dex)\n",
    "decoder_lstm = tf.keras.layers.LSTM(lstm_dim, return_sequences=True, return_state=True)\n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm(final_dex, initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = tf.keras.layers.Dense(vocabulary_size, activation='softmax') \n",
    "\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAKING THE DATA READY FOR TRAINING\n",
    "\n",
    "Here we simply create a dataset object by combining complete text and summaries, this dataset is then shuffled and batched, after these steps we are ready to start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullTextDataset = tf.data.Dataset.from_tensor_slices(inputTextDataset)\n",
    "summaryDataset = tf.data.Dataset.from_tensor_slices(targetSummaryDataset) # .shuffle(1000)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputTextDataset, targetSummaryDataset)).shuffle(1000)\n",
    "dataset = dataset.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING FOR ENCODER DECODER\n",
    "\n",
    "Here we train he encoder decoder model, we use this loop in order to not have to store all the one hot outputs in one large matriz considering that the vocabulary size is quiet large (almost 40000 entries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current epoch is 0\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 49) for input Tensor(\"input_10:0\", shape=(None, 49), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001700038C310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      " salamabad sadhna outbid misunderstandings damaged\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.5599 - accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.5122 - accuracy: 0.2551\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.1093 - accuracy: 0.2449\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.4990 - accuracy: 0.2526\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.7927 - accuracy: 0.2602\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.2640 - accuracy: 0.2653\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.7013 - accuracy: 0.2730\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.5743 - accuracy: 0.2602\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.6365 - accuracy: 0.2487\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.2829 - accuracy: 0.2602\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1159 - accuracy: 0.2934\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1230 - accuracy: 0.2806\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.5310 - accuracy: 0.2385\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.3703 - accuracy: 0.2487\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.8065 - accuracy: 0.2449\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.3899 - accuracy: 0.2462\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.5699 - accuracy: 0.2411\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.4242 - accuracy: 0.2296\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.4416 - accuracy: 0.2411\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.2336 - accuracy: 0.2564\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.7498 - accuracy: 0.2921\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.9938 - accuracy: 0.2691\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1526 - accuracy: 0.2423\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.8582 - accuracy: 0.2755\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.8293 - accuracy: 0.2628\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 6.8708 - accuracy: 0.2602\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.6448 - accuracy: 0.2793\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.6688 - accuracy: 0.2793\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.8477 - accuracy: 0.2653\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.7752 - accuracy: 0.2730\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.8845 - accuracy: 0.2436\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.2041 - accuracy: 0.2449\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.9742 - accuracy: 0.2640\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.9831 - accuracy: 0.2640\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.5752 - accuracy: 0.2819\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.6178 - accuracy: 0.2793\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.8881 - accuracy: 0.2500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.6876 - accuracy: 0.2602\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.8059 - accuracy: 0.2462\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.5087 - accuracy: 0.2946\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.2906 - accuracy: 0.2194\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.8274 - accuracy: 0.2640\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0033 - accuracy: 0.2474\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.9912 - accuracy: 0.2411\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4280 - accuracy: 0.2921\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.9902 - accuracy: 0.2487\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.6440 - accuracy: 0.2730\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.6170 - accuracy: 0.2628\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.8103 - accuracy: 0.2551\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.5458 - accuracy: 0.2691\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.0015 - accuracy: 0.2628\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.5353 - accuracy: 0.2704\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4641 - accuracy: 0.2959\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.5277 - accuracy: 0.2704\n",
      "1/1 [==============================] - 0s 997us/step - loss: 6.8640 - accuracy: 0.2474\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1352 - accuracy: 0.2321\n",
      "1/1 [==============================] - 0s 998us/step - loss: 6.6344 - accuracy: 0.2806\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.5251 - accuracy: 0.2793\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.5960 - accuracy: 0.2551\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.0640 - accuracy: 0.2449\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.8985 - accuracy: 0.2526\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0872 - accuracy: 0.2628\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.5656 - accuracy: 0.2487\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.5560 - accuracy: 0.2653\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.9899 - accuracy: 0.2423\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.6513 - accuracy: 0.2449\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.6734 - accuracy: 0.2551\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.5593 - accuracy: 0.2895\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.7277 - accuracy: 0.2526\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.6288 - accuracy: 0.2730\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.8556 - accuracy: 0.2691\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.5759 - accuracy: 0.2921\n",
      "1/1 [==============================] - 0s 997us/step - loss: 6.6277 - accuracy: 0.2742\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.2097 - accuracy: 0.3023\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1441 - accuracy: 0.2334\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.6499 - accuracy: 0.2895\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.7107 - accuracy: 0.2449\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.6310 - accuracy: 0.2423\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.6330 - accuracy: 0.2857\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.2164 - accuracy: 0.3048\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.8656 - accuracy: 0.2551\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.4961 - accuracy: 0.2908\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.9126 - accuracy: 0.2449\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4728 - accuracy: 0.2895\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.7697 - accuracy: 0.2666\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.6487 - accuracy: 0.2653\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.5467 - accuracy: 0.2717\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.6666 - accuracy: 0.2895\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.6002 - accuracy: 0.2704\n",
      "1/1 [==============================] - 0s 997us/step - loss: 6.6635 - accuracy: 0.2704\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.5922 - accuracy: 0.2666\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.8085 - accuracy: 0.2462\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.7413 - accuracy: 0.2640\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.9580 - accuracy: 0.2602\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.6878 - accuracy: 0.2589\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.5927 - accuracy: 0.2959\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4791 - accuracy: 0.2908\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.7629 - accuracy: 0.2436\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.5807 - accuracy: 0.2717\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.4533 - accuracy: 0.2755\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.2129 - accuracy: 0.2934\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.7174 - accuracy: 0.2474\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.6495 - accuracy: 0.2768\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.7107 - accuracy: 0.2474\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.8202 - accuracy: 0.2551\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.3669 - accuracy: 0.2972\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.2156 - accuracy: 0.3125\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.9093 - accuracy: 0.2564\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.6884 - accuracy: 0.2564\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.6579 - accuracy: 0.2615\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.6109 - accuracy: 0.2679\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.3749 - accuracy: 0.2985\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4218 - accuracy: 0.2755\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.2711 - accuracy: 0.2985\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.6853 - accuracy: 0.2602\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.6296 - accuracy: 0.2921\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.2438 - accuracy: 0.3112\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.3948 - accuracy: 0.2972\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.1713 - accuracy: 0.3163\n",
      "The current epoch is 1\n",
      " actor actor actor actor actor said said said said\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.8546 - accuracy: 0.3048\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.3307 - accuracy: 0.2806\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0928 - accuracy: 0.2883\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.5791 - accuracy: 0.2296\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.2045 - accuracy: 0.2768\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.3600 - accuracy: 0.2819\n",
      "1/1 [==============================] - 0s 997us/step - loss: 6.1379 - accuracy: 0.2972\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0006 - accuracy: 0.3074\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.3263 - accuracy: 0.2602\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.1634 - accuracy: 0.2870\n",
      "1/1 [==============================] - 0s 997us/step - loss: 6.2316 - accuracy: 0.2819\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.3248 - accuracy: 0.2640\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0260 - accuracy: 0.3036\n",
      "1/1 [==============================] - 0s 999us/step - loss: 6.3036 - accuracy: 0.2793\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.2712 - accuracy: 0.2691\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.3064 - accuracy: 0.2679\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.9078 - accuracy: 0.3253\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.5705 - accuracy: 0.2270\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1733 - accuracy: 0.2730\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.2670 - accuracy: 0.2921\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1439 - accuracy: 0.2883\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.5752 - accuracy: 0.2462\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1549 - accuracy: 0.2959\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0455 - accuracy: 0.2959\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.3030 - accuracy: 0.2653\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 5.9878 - accuracy: 0.2972\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.2145 - accuracy: 0.2895\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.2315 - accuracy: 0.2717\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.1125 - accuracy: 0.3010\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.3103 - accuracy: 0.2628\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0648 - accuracy: 0.3010\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.3480 - accuracy: 0.2666\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.6441 - accuracy: 0.2487\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.3984 - accuracy: 0.2640\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0117 - accuracy: 0.3010\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.2915 - accuracy: 0.2666\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.1774 - accuracy: 0.2870\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.2359 - accuracy: 0.2908\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.5264 - accuracy: 0.2449\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.1196 - accuracy: 0.2908\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.2325 - accuracy: 0.2857\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.5060 - accuracy: 0.2628\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.1270 - accuracy: 0.3099\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0134 - accuracy: 0.2972\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.0901 - accuracy: 0.2870\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.2235 - accuracy: 0.2832\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4393 - accuracy: 0.2564\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.4130 - accuracy: 0.2717\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.1541 - accuracy: 0.2934\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.4075 - accuracy: 0.2704\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.5487 - accuracy: 0.2462\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.0767 - accuracy: 0.2870\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1751 - accuracy: 0.2755\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.2227 - accuracy: 0.2870\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.3304 - accuracy: 0.2985\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1585 - accuracy: 0.2921\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1406 - accuracy: 0.2934\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.3878 - accuracy: 0.2679\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.0978 - accuracy: 0.2921\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.2701 - accuracy: 0.2870\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.2103 - accuracy: 0.2742\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4576 - accuracy: 0.2487\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.4261 - accuracy: 0.2755\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.2060 - accuracy: 0.2857\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.0669 - accuracy: 0.2972\n",
      "1/1 [==============================] - 0s 997us/step - loss: 6.3243 - accuracy: 0.2755\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.0612 - accuracy: 0.2921\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1234 - accuracy: 0.3036\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.3940 - accuracy: 0.2806\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.9395 - accuracy: 0.3099\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.4683 - accuracy: 0.2615\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.0401 - accuracy: 0.2806\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.4938 - accuracy: 0.2474\n",
      "1/1 [==============================] - 0s 997us/step - loss: 6.1422 - accuracy: 0.2832\n"
     ]
    }
   ],
   "source": [
    "#Inference Stage\n",
    "import numpy as np\n",
    "#encoder model\n",
    "# this function recovers the original word from a token (if there is a translation)\n",
    "def id_to_word(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "#encoder-decoder model\n",
    "encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "decoder_state_input_h = tf.keras.layers.Input(shape=(lstm_dim,))\n",
    "decoder_state_input_c = tf.keras.layers.Input(shape=(lstm_dim,))\n",
    "decoder_state_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "final_dex2 = tf.keras.layers.Embedding(vocabulary_size, embedding_size)(decoder_inputs)\n",
    "\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(final_dex2, initial_state=decoder_state_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "decoder_model = tf.keras.models.Model([decoder_inputs] + decoder_state_inputs, [decoder_outputs2] + decoder_states2)\n",
    "\n",
    "def decode_seq(input_seq):\n",
    "    state_values = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0,0] = tokenizer.texts_to_sequences(['startseq'])[0][0]\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + state_values)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0,-1,:])\n",
    "        sampled_char = id_to_word(sampled_token_index, tokenizer)\n",
    "        if sampled_char == None:\n",
    "            break\n",
    "        decoded_sentence += ' ' + sampled_char\n",
    "\n",
    "        if(sampled_char == 'endseq' or len(decoded_sentence) > max_summary_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0,0] = sampled_token_index\n",
    "\n",
    "        state_values = [h,c] \n",
    "\n",
    "    return decoded_sentence\n",
    "inputSentence = tf.keras.preprocessing.sequence.pad_sequences([tokenizer.texts_to_sequences([trainDataset[list(trainDataset.keys())[3]][1]])[0]], maxlen = max_length, padding='post')\n",
    "def train_encoder_decoder(dataset, batch_size, n_epochs = 50):\n",
    "    epochCounter = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"The current epoch is \" + str(epochCounter))\n",
    "        print(decode_seq(inputSentence))\n",
    "        epochCounter += 1\n",
    "        batchCounter = 0\n",
    "        for encoder_input_batch, decoder_input_batch in dataset:\n",
    "            batchCounter += 1\n",
    "            encoder_input_batch = tf.reshape(encoder_input_batch, [encoder_input_batch.get_shape()[0], encoder_input_batch.get_shape()[2]])\n",
    "            decoder_input_batch = tf.reshape(decoder_input_batch, [decoder_input_batch.get_shape()[0], decoder_input_batch.get_shape()[2]])\n",
    "            decoder_output = list()\n",
    "            for i in range(encoder_input_batch.get_shape()[0]):\n",
    "                for j in range(max_summary_length):\n",
    "                    if j == 0:\n",
    "                        decoder_output.append(list())  \n",
    "                        continue\n",
    "                    decoder_output[i].append(decoder_input_batch[i][j])\n",
    "                decoder_output[i].append(0)\n",
    "            decoder_output = tf.one_hot(decoder_output, vocabulary_size)\n",
    "            model.fit([encoder_input_batch, decoder_input_batch], decoder_output, verbose=1)\n",
    "                 \n",
    "# train the GAN model\n",
    "        \n",
    "train_encoder_decoder(dataset, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"encoder_decoder.h5\")\n",
    "trainedModel = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAN TRAINING\n",
    "\n",
    "Here is our attempt at training the GAN model, we start by generating summaries for the current batch using the generator, we then use this summaries (which will have label 0) and the real summaries (which will have label 1) to train the discriminator. Afterwards we set the discriminator to not trainable and we train the entire GAN (only the weights of the generator can change) using labels 1 for the outputs of the genarator, therefore trying to make the generator convince the discriminator that the output that it is producing are real summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(dataset, batch_size, n_epochs = 5):\n",
    "    generator, discriminator = gan.layers\n",
    "    epochCounter = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"The current epoch is \" + str(epochCounter))\n",
    "        epochCounter += 1\n",
    "        currentBatch = 0\n",
    "        for X_batch, Y_batch in dataset:\n",
    "            # phase 1 training the discriminator\n",
    "            # noise = tf.random.normal(shape=[batch_size, codings_size])\n",
    "            #print(\"The current batch is \" + str(currentBatch))\n",
    "            currentBatch += 1\n",
    "            X_batch = tf.reshape(X_batch, [X_batch.get_shape()[0], X_batch.get_shape()[2]])\n",
    "            X_batch = tf.one_hot(X_batch, vocabulary_size)\n",
    "            generated_reviews = generator(X_batch) # substitute noise with current x_input\n",
    "            summary = tf.reshape(generated_reviews[0], [generated_reviews[0].get_shape()[0], generated_reviews[0].get_shape()[1]])\n",
    "            Y_batch = tf.reshape(Y_batch, [Y_batch.get_shape()[0], Y_batch.get_shape()[2]])\n",
    "            Y_batch = tf.one_hot(Y_batch, vocabulary_size)\n",
    "            X_fake_and_real = tf.concat( [generated_reviews, Y_batch], axis=0)\n",
    "            y1 = tf.constant([[0.]] * int(X_fake_and_real.get_shape()[0] / 2) + [[1.]] * int(X_fake_and_real.get_shape()[0] / 2) )\n",
    "            discriminator.trainable = True\n",
    "            discriminator.train_on_batch(X_fake_and_real, y1)\n",
    "            # phase 2 training the generator\n",
    "            y2 = tf.constant([[1.]] * int(X_fake_and_real.get_shape()[0] / 2))\n",
    "            discriminator.trainable = False\n",
    "            gan.train_on_batch(X_batch, y2) # substitute noise with the corrent x_input\n",
    "                 \n",
    "# train the GAN model\n",
    "train_gan(dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.save('finalGan.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator, discriminator = gan.layers\n",
    "generator.save('finalGenerator.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startseq bollywood actor taapsee pannu will soon be seen in a completely different avatar in her upcoming film judwaa 2 even as the pink star shoots for the film opposite varun dhawan she has signed anubhav sinhas next which is a social thriller titled mulk the film will also feature rishi kapoor and taapsee plays the role of his daughterinlaw in the movie the shooting of mulk will begin in a few months and will be shot in lucknow and varanasifrom a superhero to vulnerable human beings in a small town in my country mulk a space i grew up in but never visited as a film maker anubhav sinha anubhavsinha july 28 2017mulk is a story inspired by true events about a joint family that is involved in a controversy and taapsees character is helping the family reclaim their honourfollow htshowbiz for more endseq\n",
      "startseq taimur ali khan is undoubtedly one of the cutest star kids on the block the fivemonthold toddler enjoys as much fan following as his mother kareena kapoor khan ever since his birth in december last year the tiny tot has become a favourite with the netizens if his adorable pictures make many hearts melt his cute antics steal the show at various eventsknown for his handsome looks taimur has already won the hearts of many and the star kid is busy stirring up a storm on the internet once again a new picture of him has caught the fancy of many clad in a stripped tshirt the tiny tot can be seen smiling for the camera only a few days ago taimurs pictures with mom kareena went viral when he attended the birthday bash of tusshar kapoors son interestingly taimur grabbed more eyeballs than the birthday boy himself going by his recent pictures he looks almost a spitting image of kareena on the work front kareena is set to start shooting for veere di wedding which marks her comeback post pregnancy saif meanwhile is gearing up for the release of chef in july endseq\n"
     ]
    }
   ],
   "source": [
    "testKeys = list()\n",
    "for key in testDataset.keys():\n",
    "    if len(testDataset[key][0]) < 300 and len(testDataset[key][1]) > 20:\n",
    "        testKeys.append(key)\n",
    "        print(testDataset[key][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "The generated summary is:  to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to\n"
     ]
    }
   ],
   "source": [
    "# this function recovers the original word from a token (if there is a translation)\n",
    "def word_to_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "generator = tf.keras.models.load_model('finalGenerator.h5')\n",
    "inputSentence = tf.keras.preprocessing.sequence.pad_sequences([tokenizer.texts_to_sequences([testDataset[list(testDataset.keys())[3]][0]])[0]], maxlen = max_length, padding='post')\n",
    "summary = generator(inputSentence) # generate summary\n",
    "summary = tf.reshape(summary, [summary.get_shape()[1], summary.get_shape()[2]])\n",
    "finalSummary = \"\"\n",
    "for i in range(len(summary)):\n",
    "    currentWord = word_to_id(argmax(summary[i]), tokenizer)\n",
    "    if currentWord==\"startseq\":\n",
    "        print(\"hello\")\n",
    "        continue\n",
    "    elif currentWord==\"endseq\":\n",
    "        print(\"hello\")\n",
    "        break\n",
    "    else:\n",
    "        finalSummary = finalSummary + \" \" + currentWord\n",
    "print(\"The generated summary is: \" + finalSummary)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUNNING INFERENCE\n",
    "\n",
    "We use the encoder decoder architecture trained previously to run inference on an input sequence of words (which is padded), we use a greedy approach to picking the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startseq\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-46df842166b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdecoded_sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[0minputSentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtestDataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecode_seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputSentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-89-46df842166b5>\u001b[0m in \u001b[0;36mdecode_seq\u001b[1;34m(input_seq)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msampled_token_index\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid_to_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampled_token_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0msampled_char\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mid_to_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampled_token_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msampled_char\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-89-46df842166b5>\u001b[0m in \u001b[0;36mid_to_word\u001b[1;34m(integer, tokenizer)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mid_to_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minteger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0minteger\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Inference Stage\n",
    "import numpy as np\n",
    "#encoder model\n",
    "# this function recovers the original word from a token (if there is a translation)\n",
    "def id_to_word(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "#encoder-decoder model\n",
    "encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "decoder_state_input_h = tf.keras.layers.Input(shape=(lstm_dim,))\n",
    "decoder_state_input_c = tf.keras.layers.Input(shape=(lstm_dim,))\n",
    "decoder_state_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "final_dex2 = tf.keras.layers.Embedding(max_summary_length, embedding_size)(decoder_inputs)\n",
    "\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(final_dex2, initial_state=decoder_state_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "decoder_model = tf.keras.models.Model([decoder_inputs] + decoder_state_inputs, [decoder_outputs2] + decoder_states2)\n",
    "\n",
    "def decode_seq(input_seq):\n",
    "    state_values = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0,0] = tokenizer.texts_to_sequences(['startseq'])[0][0]\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + state_values)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0,-1,:])\n",
    "        if sampled_token_index != 0:\n",
    "            print(id_to_word(sampled_token_index, tokenizer))\n",
    "        sampled_char = id_to_word(sampled_token_index, tokenizer)\n",
    "        if sampled_char == None:\n",
    "            continue\n",
    "        decoded_sentence += ' ' + sampled_char\n",
    "\n",
    "        if(sampled_char == 'endseq' or len(decoded_sentence) > max_summary_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0,0] = sampled_token_index\n",
    "\n",
    "        state_values = [h,c] \n",
    "\n",
    "    return decoded_sentence\n",
    "inputSentence = tf.keras.preprocessing.sequence.pad_sequences([tokenizer.texts_to_sequences([testDataset[list(testDataset.keys())[3]][0]])[0]], maxlen = max_length, padding='post')\n",
    "print(decode_seq(inputSentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVALUATION \n",
    "\n",
    "We evaluate the dataset using BLEU-1 score and we do this by using the inference function described above. We ran evaluation using the test dataset which we set aside from the entire dataset in the beginning of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-167-c458c929c6a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Bleu-1 score: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_bleu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mevaluate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-167-c458c929c6a1>\u001b[0m in \u001b[0;36mevaluate_model\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtexts\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;31m# prediction for current image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecode_seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'post'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[1;31m# get the correct descriptions and remove the startseq and endseq tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mcorrectSummary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-160-d962c0c4900a>\u001b[0m in \u001b[0;36mdecode_seq\u001b[1;34m(input_seq)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstop_condition\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0moutput_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget_seq\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstate_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0msampled_token_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[0;32m    129\u001b[0m           method.__name__))\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1612\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'outputs'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_outputs\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1614\u001b[1;33m     \u001b[0mall_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure_up_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1615\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure_up_to\u001b[1;34m(shallow_tree, func, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0mshallow_tree\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m   \"\"\"\n\u001b[1;32m-> 1135\u001b[1;33m   return map_structure_with_tuple_paths_up_to(\n\u001b[0m\u001b[0;32m   1136\u001b[0m       \u001b[0mshallow_tree\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[1;32mlambda\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Discards the path arg.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure_with_tuple_paths_up_to\u001b[1;34m(shallow_tree, func, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1232\u001b[0m   flat_path_list = [path for path, _\n\u001b[0;32m   1233\u001b[0m                     in _yield_flat_up_to(shallow_tree, inputs[0], is_seq)]\n\u001b[1;32m-> 1234\u001b[1;33m   results = [func(*args, **kwargs) for args in zip(flat_path_list,\n\u001b[0m\u001b[0;32m   1235\u001b[0m                                                    *flat_value_lists)]\n\u001b[0;32m   1236\u001b[0m   return pack_sequence_as(structure=shallow_tree, flat_sequence=results,\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1232\u001b[0m   flat_path_list = [path for path, _\n\u001b[0;32m   1233\u001b[0m                     in _yield_flat_up_to(shallow_tree, inputs[0], is_seq)]\n\u001b[1;32m-> 1234\u001b[1;33m   results = [func(*args, **kwargs) for args in zip(flat_path_list,\n\u001b[0m\u001b[0;32m   1235\u001b[0m                                                    *flat_value_lists)]\n\u001b[0;32m   1236\u001b[0m   return pack_sequence_as(structure=shallow_tree, flat_sequence=results,\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(_, *values)\u001b[0m\n\u001b[0;32m   1135\u001b[0m   return map_structure_with_tuple_paths_up_to(\n\u001b[0;32m   1136\u001b[0m       \u001b[0mshallow_tree\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1137\u001b[1;33m       \u001b[1;32mlambda\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Discards the path arg.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1138\u001b[0m       \u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       **kwargs)\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(tensors, axis)\u001b[0m\n\u001b[0;32m   2671\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mragged_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRaggedTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2672\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mragged_concat_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2673\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(values, axis, name)\u001b[0m\n\u001b[0;32m   1651\u001b[0m           \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"concat_dim\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m           dtype=dtypes.int32).get_shape().assert_has_rank(0)\n\u001b[1;32m-> 1653\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1654\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1655\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m    285\u001b[0m     \u001b[1;31m# variables. Variables have correct handle data when graph building.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m     \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m   \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m   \u001b[1;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_handle_data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m   3985\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3986\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3987\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   3988\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Identity\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3989\u001b[0m         tld.op_callbacks, input)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def evaluate_model(dataset):\n",
    "    # actual keeps track of correct sequence while predicted is the predicted sequence\n",
    "    actual, predicted = list(), list()\n",
    "    # looping through every image in dataset\n",
    "    for key, texts in dataset.items():\n",
    "        # prediction for current image\n",
    "        prediction = decode_seq(tf.keras.preprocessing.sequence.pad_sequences([tokenizer.texts_to_sequences([texts[1]])[0]], maxlen = max_length, padding='post'))\n",
    "        # get the correct descriptions and remove the startseq and endseq tokens\n",
    "        correctSummary = texts[0].split()[1:-1]\n",
    "        # append the prediction and the correct description to the lists\n",
    "        actual.append(correctSummary)\n",
    "        predicted.append(prediction.split())\n",
    "    # calculate BLEU score for 1 ,2 ,3 and 4 GRAM\n",
    "    print(\"Bleu-1 score: \" + str(corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))))\n",
    "    \n",
    "evaluate_model(testDataset)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
